{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38f6e294-f768-443b-8c38-8690568e54a4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Securely Mounting Azure Data Lake Storage Gen2 in Databricks Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d6782b3-a56b-460e-a142-c40ef41ea219",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to securely mount Azure Data Lake Storage Gen2 (ADLS Gen2) containers to a Databricks workspace. It covers the setup of OAuth authentication, dynamic configuration through widgets, and error handling during the mounting process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5e10566-beff-4850-8791-63b1df2892dc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Guide\n",
    "\n",
    "Before proceeding with data loading and inspection in Spark, ensure access to Azure Data Lake Storage Gen2 is configured using a Service Principal for secure application and service authentication with Azure resources.\n",
    "\n",
    "**Key Steps**\n",
    "\n",
    "- 1. Register a Microsoft Entra ID Application: Create an application identity in Azure for integration.\n",
    "- 2. Generate Client Secret: Obtain a secret for application authentication.\n",
    "- 3. Assign Permissions: Grant access roles to the service principal for data lake access.\n",
    "- 4. Secure Credential Storage: Use Azure Key Vault for storing the client secret.\n",
    "- 5. Configure Databricks Workspace: Set up OAuth 2.0 authentication in Databricks with the service principal credentials.\n",
    "\n",
    "Refer to [Microsoft Documentation](https://learn.microsoft.com/en-us/azure/databricks/connect/storage/tutorial-azure-storage) for detailed guidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7345abe-650a-4fb3-80df-fdcd957eafa3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "- Access to an Azure subscription and permissions to manage resources.\n",
    "- An Azure Data Lake Storage Gen2 account configured with appropriate access controls.\n",
    "- A Databricks workspace with necessary permissions to create and manage clusters, notebooks, and secret scopes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4194a8a2-a3c7-4d80-a895-15d83d24b1b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c3e1063-e3dc-4fea-afe5-0b5a347152a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Setup Secret Scope\n",
    "\n",
    "First, store your Azure credentials (DNS Name and Resource ID) in a Databricks secret scope. This keeps your sensitive information secure.\n",
    "\n",
    "List all the secret scopes to confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2c1826d-4e72-449a-aa28-6dcfb9c0ba1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[SecretScope(name='secretScopeDemo')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all secret scopes\n",
    "\n",
    "dbutils.secrets.listScopes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9152042-59bc-4359-923d-d7df651791be",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Configure Authentication\n",
    "\n",
    "In your Databricks notebook, define the authentication parameters. Use widgets to allow dynamic specification of the secret scope containing your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94f0afa-c370-4654-a8b0-6ff95bb17a28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure access to Azure Data Lake Storage Gen2 using OAuth authentication by setting up necessary configurations and retrieving sensitive credentials securely via Databricks' secret scopes.\n",
    "\n",
    "dbutils.widgets.text('keyvault_scope', 'secretScopeDemo')\n",
    "keyvault_scope = dbutils.widgets.get('keyvault_scope')\n",
    "\n",
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": dbutils.secrets.get(keyvault_scope, \"applicationID\"),\n",
    "    \"fs.azure.account.oauth2.client.secret\": dbutils.secrets.get(keyvault_scope, \"secretSPValue\"),\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/{tenant-id}/oauth2/token\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55807576-8a5f-4008-99c1-168f2cc12605",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "_NOTE_: Replace {tenant-id} with your actual Azure tenant ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06f327fb-106b-40c2-bd1e-3babe193955d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Mount the Data Lakes\n",
    "Unmount any existing mount at the specified point, then mount the ADLS Gen2 filesystem for all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c5a97a9-e78c-438e-bf80-b15af5f865d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/base/ has been unmounted.\n/mnt/bronze/ has been unmounted.\n/mnt/silver/ has been unmounted.\n/mnt/gold/ has been unmounted.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BASE\n",
    "# Unmount if exists, then mount Azure Data Lake Storage Gen2 at 'baseMountPoint' with 'configs'.\n",
    "\n",
    "source = \"abfss://base@sadbhgtsh.dfs.core.windows.net/\"\n",
    "baseMountPoint = \"/mnt/base/\"\n",
    "try:\n",
    "    dbutils.fs.unmount(baseMountPoint)\n",
    "except:\n",
    "    print(\"count not unmount base storage\")\n",
    "    pass\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=source,\n",
    "    mount_point=baseMountPoint,\n",
    "    extra_configs=configs\n",
    ")\n",
    "\n",
    "# BRONZE\n",
    "# Unmount if exists, then mount Azure Data Lake Storage Gen2 at 'bronzeMountPoint' with 'configs'.\n",
    "\n",
    "source = \"abfss://bronze@sadbhgtsh.dfs.core.windows.net/\"\n",
    "bronzeMountPoint = \"/mnt/bronze/\"\n",
    "try:\n",
    "    dbutils.fs.unmount(bronzeMountPoint)\n",
    "except:\n",
    "    print(\"count not unmount bronze storage\")\n",
    "    pass\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=source,\n",
    "    mount_point=bronzeMountPoint,\n",
    "    extra_configs=configs\n",
    ")\n",
    "\n",
    "# SILVER\n",
    "# Unmount if exists, then mount Azure Data Lake Storage Gen2 at 'silverMountPoint' with 'configs'.\n",
    "\n",
    "source = \"abfss://silver@sadbhgtsh.dfs.core.windows.net/\"\n",
    "silverMountPoint = \"/mnt/silver/\"\n",
    "try:\n",
    "    dbutils.fs.unmount(silverMountPoint)\n",
    "except:\n",
    "    print(\"count not unmount silver storage\")\n",
    "    pass\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=source,\n",
    "    mount_point=silverMountPoint,\n",
    "    extra_configs=configs\n",
    ")\n",
    "\n",
    "# GOLD\n",
    "# Unmount if exists, then mount Azure Data Lake Storage Gen2 at 'goldMountPoint' with 'configs'.\n",
    "\n",
    "source = \"abfss://gold@sadbhgtsh.dfs.core.windows.net/\"\n",
    "goldMountPoint = \"/mnt/gold/\"\n",
    "try:\n",
    "    dbutils.fs.unmount(goldMountPoint)\n",
    "except:\n",
    "    print(\"count not unmount gold storage\")\n",
    "    pass\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=source,\n",
    "    mount_point=goldMountPoint,\n",
    "    extra_configs=configs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "663a8bf9-4bd3-48e5-b55e-c252a91e320f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Exit Notebook with Mount Points Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ee885a-3079-4542-90ec-f5c3ae8630be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Exit the notebook and return the mount points for the base, bronze, silver and gold data lakes as a JSON string.\n",
    "\n",
    "dbutils.notebook.exit(str({\"baseMountPoint\": baseMountPoint, \"bronzeMountPoint\": bronzeMountPoint, \"silverMountPoint\": silverMountPoint, \"goldMountPoint\": goldMountPoint}))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-config",
   "widgets": {
    "keyvault_scope": {
     "currentValue": "secretScopeDemo",
     "nuid": "7f2adb95-1f9e-4f63-a038-8312676adc81",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "secretScopeDemo",
      "label": null,
      "name": "keyvault_scope",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
